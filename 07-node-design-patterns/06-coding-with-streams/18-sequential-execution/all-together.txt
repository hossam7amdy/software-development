import { createReadStream, createWriteStream } from 'fs'
import Stream, { Readable, Transform, type TransformCallback } from 'stream'

export const concatFiles = (dest: string, files: string[]): Promise<void> => {
  return new Promise((resolve, rejects) => {
    const destStream = createWriteStream(dest)
    Readable.from(files)
      .pipe(
        new Transform({
          objectMode: true,
          transform: (
            filename: string,
            _enc: BufferEncoding,
            done: TransformCallback
          ) => {
            const src = createReadStream(filename)
            src.pipe(destStream, { end: false })
            src.on('error', done)
            src.on('end', done)
          }
        })
      )
      .on('error', rejects)
      .on('finish', () => {
        destStream.end()
        resolve()
      })
  })
}
import { concatFiles } from './concat-files'

async function main() {
  try {
    await concatFiles(process.argv[2], process.argv.slice(3))
  } catch (e) {
    console.error(e)
    process.exit(1)
  }

  console.log('All files concatenated successfully')
}

main()
{
  "name": "18-sequential-execution",
  "version": "1.0.0",
  "description": "",
  "main": "concat-files.js",
  "type": "module",
  "scripts": {
    "test": "echo \"Error: no test specified\" && exit 1"
  },
  "keywords": [],
  "author": "",
  "license": "ISC",
  "engines": {
    "node": ">=14"
  },
  "engineStrict": true
}
# 18-sequential-execution

This examples shows how to create a sequential execution flow using streams.


## Run

To run the example:

```bash
node concat.js <destination> <source1> <source2> <source3> ...
```
